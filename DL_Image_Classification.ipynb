{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b0b04445-fe49-4ee0-a99e-7d46af078985",
      "metadata": {
        "tags": [],
        "id": "b0b04445-fe49-4ee0-a99e-7d46af078985",
        "outputId": "fe0b6853-43d7-46c0-8105-cd459f688446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import seaborn as sb\n",
        "import cv2\n",
        "os.system(\"rm /content/archive-4/train/.DS_Store\")\n",
        "os.system(\"rm /content/archive-4/test/.DS_Store\")\n",
        "os.system(\"rm /content/archive-4/valid/.DS_Store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will now import and standardize the data."
      ],
      "metadata": {
        "id": "Hj0J9lSoQOXo"
      },
      "id": "Hj0J9lSoQOXo"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db62369c-ecae-40db-8b6c-ff50c933e9c0",
      "metadata": {
        "tags": [],
        "id": "db62369c-ecae-40db-8b6c-ff50c933e9c0"
      },
      "outputs": [],
      "source": [
        "global labels\n",
        "labels=os.listdir(\"/content/archive-4/train\")\n",
        "\n",
        "def getdataset(datapath):\n",
        "    global labels\n",
        "    imlen = 224\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        npath=os.path.join(datapath,label)\n",
        "        nnum=labels.index(label)\n",
        "        for image in os.listdir(npath):\n",
        "            imageArray = cv2.imread(os.path.join(npath,image))[::-1]\n",
        "            resized_arr = cv2.resize(imageArray, (imlen, imlen))\n",
        "            if (len(resized_arr) == 224 and len(resized_arr[0]) == 224) and len(resized_arr[0][0]):\n",
        "                data.append([resized_arr,nnum])\n",
        "            else:\n",
        "                raise ValueError\n",
        "    return data\n",
        "\n",
        "train = getdataset(\"/content/archive-4/train\")\n",
        "validation = getdataset(\"/content/archive-4/valid\")\n",
        "test = getdataset(\"/content/archive-4/test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a graph counting the occurrences of each species given their number of appearance in the data. "
      ],
      "metadata": {
        "id": "TVi_NlgmQTp7"
      },
      "id": "TVi_NlgmQTp7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3cb1a972-25f6-44f6-8bce-a2a44d83300e",
      "metadata": {
        "tags": [],
        "id": "3cb1a972-25f6-44f6-8bce-a2a44d83300e",
        "outputId": "4b5765a5-20a5-446e-ed22-b65b001447ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "l=list([x[1] for x in train])\n",
        "sb.histplot(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown by the graph above, this dataset contains around 120 images for each of the 100 species of butterfly or moth in the dataset within the training data.\n",
        "Given the data, this model should be able to identify the species of a butterfly or a moth given its picture in the given format, as long as it is within the 100 species that are in the training dataset."
      ],
      "metadata": {
        "id": "Lr0zdGnAPo5x"
      },
      "id": "Lr0zdGnAPo5x"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "833677ff-5e01-4d4f-8424-12fcd0e066da",
      "metadata": {
        "tags": [],
        "id": "833677ff-5e01-4d4f-8424-12fcd0e066da"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_val = []\n",
        "y_val = []\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "\n",
        "for f1 in train:\n",
        "    feature=f1[0]\n",
        "    label=f1[1]\n",
        "    x_train.append(feature)\n",
        "    y_train.append(label)\n",
        "\n",
        "for f2 in validation:\n",
        "    feature=f2[0]\n",
        "    label=f2[1]\n",
        "    x_val.append(feature)\n",
        "    y_val.append(label)\n",
        "    \n",
        "for f3 in test:\n",
        "    feature=f3[0]\n",
        "    label=f3[1]\n",
        "    x_test.append(feature)\n",
        "    y_test.append(label)\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "y_train=keras.utils.to_categorical(np.array(y_train))\n",
        "x_val=np.array(x_val)\n",
        "y_val=keras.utils.to_categorical(np.array(y_val))\n",
        "x_test=np.array(x_test)\n",
        "y_test=keras.utils.to_categorical(np.array(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we begin working with two model architectures to identify the moths or butterflies."
      ],
      "metadata": {
        "id": "WhTP6qbjQleM"
      },
      "id": "WhTP6qbjQleM"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "562a528e-83a1-4241-8b9b-ee6b505eb3e6",
      "metadata": {
        "id": "562a528e-83a1-4241-8b9b-ee6b505eb3e6",
        "outputId": "3520304a-c6cf-4de6-ecb7-38ae29513d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 56, 56, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 28, 28, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               802944    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 927,044\n",
            "Trainable params: 927,044\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "394/394 [==============================] - 24s 34ms/step - loss: 4.4618 - accuracy: 0.0250 - val_loss: 3.9146 - val_accuracy: 0.0620\n",
            "Epoch 2/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 3.6523 - accuracy: 0.0985 - val_loss: 3.2565 - val_accuracy: 0.1480\n",
            "Epoch 3/50\n",
            "394/394 [==============================] - 12s 31ms/step - loss: 2.9644 - accuracy: 0.2244 - val_loss: 2.5998 - val_accuracy: 0.2940\n",
            "Epoch 4/50\n",
            "394/394 [==============================] - 13s 32ms/step - loss: 2.3904 - accuracy: 0.3518 - val_loss: 2.2010 - val_accuracy: 0.3960\n",
            "Epoch 5/50\n",
            "394/394 [==============================] - 12s 31ms/step - loss: 1.9830 - accuracy: 0.4370 - val_loss: 1.9783 - val_accuracy: 0.4420\n",
            "Epoch 6/50\n",
            "394/394 [==============================] - 12s 31ms/step - loss: 1.6262 - accuracy: 0.5312 - val_loss: 1.9083 - val_accuracy: 0.4700\n",
            "Epoch 7/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 1.3427 - accuracy: 0.6053 - val_loss: 1.7770 - val_accuracy: 0.5540\n",
            "Epoch 8/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 1.1393 - accuracy: 0.6582 - val_loss: 1.8638 - val_accuracy: 0.5500\n",
            "Epoch 9/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.9620 - accuracy: 0.7111 - val_loss: 2.1989 - val_accuracy: 0.5340\n",
            "Epoch 10/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.8272 - accuracy: 0.7497 - val_loss: 2.0640 - val_accuracy: 0.5300\n",
            "Epoch 11/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.7141 - accuracy: 0.7822 - val_loss: 2.1819 - val_accuracy: 0.5360\n",
            "Epoch 12/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.6528 - accuracy: 0.8023 - val_loss: 2.2029 - val_accuracy: 0.5380\n",
            "Epoch 13/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.5446 - accuracy: 0.8349 - val_loss: 2.1010 - val_accuracy: 0.5860\n",
            "Epoch 14/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.5313 - accuracy: 0.8383 - val_loss: 2.4437 - val_accuracy: 0.5620\n",
            "Epoch 15/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.4859 - accuracy: 0.8573 - val_loss: 2.3873 - val_accuracy: 0.5740\n",
            "Epoch 16/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.4238 - accuracy: 0.8722 - val_loss: 2.5858 - val_accuracy: 0.5520\n",
            "Epoch 17/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3720 - accuracy: 0.8896 - val_loss: 2.6152 - val_accuracy: 0.5760\n",
            "Epoch 18/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3956 - accuracy: 0.8843 - val_loss: 2.5055 - val_accuracy: 0.5560\n",
            "Epoch 19/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3597 - accuracy: 0.8964 - val_loss: 2.6001 - val_accuracy: 0.5820\n",
            "Epoch 20/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3238 - accuracy: 0.9053 - val_loss: 2.4857 - val_accuracy: 0.5660\n",
            "Epoch 21/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3433 - accuracy: 0.9030 - val_loss: 2.5267 - val_accuracy: 0.5300\n",
            "Epoch 22/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3005 - accuracy: 0.9144 - val_loss: 3.0214 - val_accuracy: 0.5680\n",
            "Epoch 23/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3182 - accuracy: 0.9082 - val_loss: 2.8637 - val_accuracy: 0.5120\n",
            "Epoch 24/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3019 - accuracy: 0.9211 - val_loss: 2.7859 - val_accuracy: 0.5520\n",
            "Epoch 25/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2544 - accuracy: 0.9286 - val_loss: 3.1251 - val_accuracy: 0.5440\n",
            "Epoch 26/50\n",
            "394/394 [==============================] - 11s 29ms/step - loss: 0.2763 - accuracy: 0.9232 - val_loss: 3.0861 - val_accuracy: 0.5540\n",
            "Epoch 27/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2855 - accuracy: 0.9246 - val_loss: 3.1869 - val_accuracy: 0.5480\n",
            "Epoch 28/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2181 - accuracy: 0.9374 - val_loss: 2.8449 - val_accuracy: 0.5060\n",
            "Epoch 29/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2575 - accuracy: 0.9288 - val_loss: 2.6481 - val_accuracy: 0.5440\n",
            "Epoch 30/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2449 - accuracy: 0.9339 - val_loss: 2.8472 - val_accuracy: 0.5300\n",
            "Epoch 31/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2157 - accuracy: 0.9417 - val_loss: 3.1575 - val_accuracy: 0.5400\n",
            "Epoch 32/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2465 - accuracy: 0.9356 - val_loss: 2.9494 - val_accuracy: 0.5340\n",
            "Epoch 33/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2353 - accuracy: 0.9387 - val_loss: 2.6987 - val_accuracy: 0.5920\n",
            "Epoch 34/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2258 - accuracy: 0.9397 - val_loss: 3.1982 - val_accuracy: 0.5540\n",
            "Epoch 35/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2080 - accuracy: 0.9457 - val_loss: 3.1328 - val_accuracy: 0.5280\n",
            "Epoch 36/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1913 - accuracy: 0.9499 - val_loss: 3.0884 - val_accuracy: 0.5560\n",
            "Epoch 37/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1811 - accuracy: 0.9543 - val_loss: 3.4198 - val_accuracy: 0.5340\n",
            "Epoch 38/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.1578 - accuracy: 0.9583 - val_loss: 3.1341 - val_accuracy: 0.5220\n",
            "Epoch 39/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2466 - accuracy: 0.9358 - val_loss: 3.2955 - val_accuracy: 0.5660\n",
            "Epoch 40/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2419 - accuracy: 0.9420 - val_loss: 3.4802 - val_accuracy: 0.5440\n",
            "Epoch 41/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1863 - accuracy: 0.9505 - val_loss: 3.3242 - val_accuracy: 0.5300\n",
            "Epoch 42/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2155 - accuracy: 0.9434 - val_loss: 3.1052 - val_accuracy: 0.5580\n",
            "Epoch 43/50\n",
            "394/394 [==============================] - 11s 29ms/step - loss: 0.1789 - accuracy: 0.9532 - val_loss: 3.2570 - val_accuracy: 0.5340\n",
            "Epoch 44/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1858 - accuracy: 0.9515 - val_loss: 3.3015 - val_accuracy: 0.5320\n",
            "Epoch 45/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1757 - accuracy: 0.9559 - val_loss: 3.4653 - val_accuracy: 0.5600\n",
            "Epoch 46/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1891 - accuracy: 0.9557 - val_loss: 3.4814 - val_accuracy: 0.5360\n",
            "Epoch 47/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1840 - accuracy: 0.9555 - val_loss: 3.2866 - val_accuracy: 0.5540\n",
            "Epoch 48/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1591 - accuracy: 0.9613 - val_loss: 3.7170 - val_accuracy: 0.5380\n",
            "Epoch 49/50\n",
            "394/394 [==============================] - 11s 29ms/step - loss: 0.1996 - accuracy: 0.9497 - val_loss: 3.2041 - val_accuracy: 0.5520\n",
            "Epoch 50/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1878 - accuracy: 0.9550 - val_loss: 3.4011 - val_accuracy: 0.5300\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model.add(tf.keras.layers.MaxPool2D())\n",
        "model.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model.add(tf.keras.layers.MaxPool2D())\n",
        "model.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model.add(tf.keras.layers.MaxPool2D())\n",
        "model.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model.add(tf.keras.layers.MaxPool2D())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(len(labels), activation=\"softmax\"))\n",
        "model.summary()\n",
        "#print(x_val.shape,y_val.shape)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer = opt , \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "history = model.fit(x_train,y_train,epochs = 50 , validation_data = (x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model2.add(tf.keras.layers.MaxPool2D())\n",
        "model2.add(tf.keras.layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
        "model2.add(tf.keras.layers.MaxPool2D())\n",
        "\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dropout(0.4))\n",
        "model2.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "model2.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
        "\n",
        "model2.add(tf.keras.layers.Dense(len(labels), activation=\"softmax\"))\n",
        "model2.summary()\n",
        "#print(x_val.shape,y_val.shape)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model2.compile(optimizer = opt , \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "history = model2.fit(x_train,y_train,epochs = 50 , validation_data = (x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKDjHkVJm2GH",
        "outputId": "f5d10370-c79e-4e3f-e56c-bd2b08749cf6"
      },
      "id": "LKDjHkVJm2GH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 112, 112, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 112, 112, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 56, 56, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 100352)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100352)            0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               12845184  \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,884,740\n",
            "Trainable params: 12,884,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "394/394 [==============================] - 13s 32ms/step - loss: 15.9606 - accuracy: 0.0270 - val_loss: 4.2316 - val_accuracy: 0.0840\n",
            "Epoch 2/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 3.9294 - accuracy: 0.1121 - val_loss: 3.8522 - val_accuracy: 0.1200\n",
            "Epoch 3/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 3.1782 - accuracy: 0.2473 - val_loss: 4.0430 - val_accuracy: 0.1460\n",
            "Epoch 4/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 2.4018 - accuracy: 0.4151 - val_loss: 3.8451 - val_accuracy: 0.1740\n",
            "Epoch 5/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 1.7612 - accuracy: 0.5613 - val_loss: 4.4897 - val_accuracy: 0.2080\n",
            "Epoch 6/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 1.3370 - accuracy: 0.6659 - val_loss: 4.7890 - val_accuracy: 0.1940\n",
            "Epoch 7/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 1.0810 - accuracy: 0.7249 - val_loss: 5.0338 - val_accuracy: 0.2060\n",
            "Epoch 8/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.8954 - accuracy: 0.7783 - val_loss: 6.0762 - val_accuracy: 0.1960\n",
            "Epoch 9/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.7256 - accuracy: 0.8186 - val_loss: 6.6165 - val_accuracy: 0.2180\n",
            "Epoch 10/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.6357 - accuracy: 0.8448 - val_loss: 6.2802 - val_accuracy: 0.2040\n",
            "Epoch 11/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.5965 - accuracy: 0.8572 - val_loss: 7.0893 - val_accuracy: 0.1660\n",
            "Epoch 12/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.5497 - accuracy: 0.8745 - val_loss: 7.5881 - val_accuracy: 0.2180\n",
            "Epoch 13/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.4812 - accuracy: 0.8822 - val_loss: 7.2021 - val_accuracy: 0.1880\n",
            "Epoch 14/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.4651 - accuracy: 0.8926 - val_loss: 7.1815 - val_accuracy: 0.2140\n",
            "Epoch 15/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.5061 - accuracy: 0.8896 - val_loss: 7.9525 - val_accuracy: 0.1820\n",
            "Epoch 16/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.4348 - accuracy: 0.9049 - val_loss: 8.5394 - val_accuracy: 0.2260\n",
            "Epoch 17/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3790 - accuracy: 0.9108 - val_loss: 8.0896 - val_accuracy: 0.2060\n",
            "Epoch 18/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3562 - accuracy: 0.9223 - val_loss: 8.9364 - val_accuracy: 0.2100\n",
            "Epoch 19/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3151 - accuracy: 0.9255 - val_loss: 9.1967 - val_accuracy: 0.2220\n",
            "Epoch 20/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3927 - accuracy: 0.9202 - val_loss: 8.5863 - val_accuracy: 0.2200\n",
            "Epoch 21/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3416 - accuracy: 0.9250 - val_loss: 9.4951 - val_accuracy: 0.2240\n",
            "Epoch 22/50\n",
            "394/394 [==============================] - 13s 34ms/step - loss: 0.3621 - accuracy: 0.9215 - val_loss: 9.3373 - val_accuracy: 0.2360\n",
            "Epoch 23/50\n",
            "394/394 [==============================] - 13s 34ms/step - loss: 0.3358 - accuracy: 0.9279 - val_loss: 9.6100 - val_accuracy: 0.2320\n",
            "Epoch 24/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.3788 - accuracy: 0.9262 - val_loss: 10.0738 - val_accuracy: 0.2300\n",
            "Epoch 25/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.3283 - accuracy: 0.9302 - val_loss: 8.6381 - val_accuracy: 0.2560\n",
            "Epoch 26/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2987 - accuracy: 0.9339 - val_loss: 9.8998 - val_accuracy: 0.2060\n",
            "Epoch 27/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2381 - accuracy: 0.9497 - val_loss: 9.9193 - val_accuracy: 0.2280\n",
            "Epoch 28/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2901 - accuracy: 0.9384 - val_loss: 9.3363 - val_accuracy: 0.2320\n",
            "Epoch 29/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2486 - accuracy: 0.9512 - val_loss: 10.1749 - val_accuracy: 0.2360\n",
            "Epoch 30/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2582 - accuracy: 0.9466 - val_loss: 9.2558 - val_accuracy: 0.2220\n",
            "Epoch 31/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2795 - accuracy: 0.9452 - val_loss: 10.4508 - val_accuracy: 0.2340\n",
            "Epoch 32/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2173 - accuracy: 0.9544 - val_loss: 10.8989 - val_accuracy: 0.2420\n",
            "Epoch 33/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2296 - accuracy: 0.9506 - val_loss: 12.2893 - val_accuracy: 0.2280\n",
            "Epoch 34/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2450 - accuracy: 0.9486 - val_loss: 10.6904 - val_accuracy: 0.2260\n",
            "Epoch 35/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2670 - accuracy: 0.9491 - val_loss: 10.5507 - val_accuracy: 0.2880\n",
            "Epoch 36/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.1864 - accuracy: 0.9623 - val_loss: 10.9337 - val_accuracy: 0.2360\n",
            "Epoch 37/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2248 - accuracy: 0.9557 - val_loss: 12.6483 - val_accuracy: 0.2520\n",
            "Epoch 38/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2487 - accuracy: 0.9523 - val_loss: 11.0653 - val_accuracy: 0.2740\n",
            "Epoch 39/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2243 - accuracy: 0.9565 - val_loss: 13.4816 - val_accuracy: 0.2140\n",
            "Epoch 40/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2433 - accuracy: 0.9537 - val_loss: 11.2654 - val_accuracy: 0.2680\n",
            "Epoch 41/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2256 - accuracy: 0.9572 - val_loss: 11.6863 - val_accuracy: 0.2520\n",
            "Epoch 42/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2345 - accuracy: 0.9539 - val_loss: 13.0187 - val_accuracy: 0.2380\n",
            "Epoch 43/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2458 - accuracy: 0.9595 - val_loss: 13.0893 - val_accuracy: 0.2080\n",
            "Epoch 44/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.2835 - accuracy: 0.9529 - val_loss: 12.1039 - val_accuracy: 0.2480\n",
            "Epoch 45/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1630 - accuracy: 0.9684 - val_loss: 13.9239 - val_accuracy: 0.2560\n",
            "Epoch 46/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2106 - accuracy: 0.9613 - val_loss: 12.4544 - val_accuracy: 0.2620\n",
            "Epoch 47/50\n",
            "394/394 [==============================] - 12s 30ms/step - loss: 0.1561 - accuracy: 0.9682 - val_loss: 14.5487 - val_accuracy: 0.2720\n",
            "Epoch 48/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2140 - accuracy: 0.9655 - val_loss: 13.0447 - val_accuracy: 0.2380\n",
            "Epoch 49/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.1955 - accuracy: 0.9661 - val_loss: 13.1110 - val_accuracy: 0.2660\n",
            "Epoch 50/50\n",
            "394/394 [==============================] - 12s 29ms/step - loss: 0.2061 - accuracy: 0.9643 - val_loss: 14.8803 - val_accuracy: 0.2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ec1ec672-a4ed-487e-875e-83eb97761719",
      "metadata": {
        "tags": [],
        "id": "ec1ec672-a4ed-487e-875e-83eb97761719",
        "outputId": "cc685cde-666c-4bff-c494-a58eab53fb58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 10ms/step\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "               ORANGE TIP       0.60      0.60      0.60         5\n",
            "             LARGE MARBLE       0.00      0.00      0.00         5\n",
            "        COMMON WOOD-NYMPH       0.20      0.20      0.20         5\n",
            "          CHALK HILL BLUE       1.00      0.60      0.75         5\n",
            "               PAPER KITE       0.75      0.60      0.67         5\n",
            "                    ATALA       0.80      0.80      0.80         5\n",
            "         GREEN HAIRSTREAK       0.60      0.60      0.60         5\n",
            "        BLUE SPOTTED CROW       0.00      0.00      0.00         5\n",
            "       RED SPOTTED PURPLE       0.50      0.60      0.55         5\n",
            "              GOLD BANDED       0.45      1.00      0.62         5\n",
            "        GARDEN TIGER MOTH       0.43      0.60      0.50         5\n",
            "              RED ADMIRAL       0.75      0.60      0.67         5\n",
            "         MANGROVE SKIPPER       0.67      0.80      0.73         5\n",
            "            INDRA SWALLOW       0.67      0.40      0.50         5\n",
            "                  PEACOCK       0.67      0.80      0.73         5\n",
            "          CAIRNS BIRDWING       1.00      0.60      0.75         5\n",
            "               COMET MOTH       1.00      0.40      0.57         5\n",
            "                  IO MOTH       1.00      0.80      0.89         5\n",
            "              BROWN ARGUS       0.27      0.60      0.37         5\n",
            "                  APPOLLO       0.50      0.40      0.44         5\n",
            " GREEN CELLED CATTLEHEART       0.57      0.80      0.67         5\n",
            "                  VICEROY       0.71      1.00      0.83         5\n",
            "         EMPEROR GUM MOTH       0.50      0.60      0.55         5\n",
            "   MADAGASCAN SUNSET MOTH       0.75      0.60      0.67         5\n",
            "            DANAID EGGFLY       0.14      0.20      0.17         5\n",
            "        COMMON BANDED AWL       0.17      0.20      0.18         5\n",
            "          ZEBRA LONG WING       0.83      1.00      0.91         5\n",
            " BANDED ORANGE HELICONIAN       0.33      0.40      0.36         5\n",
            "                GREAT JAY       0.67      0.80      0.73         5\n",
            "           AMERICAN SNOOT       0.00      0.00      0.00         5\n",
            "           MOURNING CLOAK       1.00      0.80      0.89         5\n",
            "       EASTERN PINE ELFIN       0.38      0.60      0.46         5\n",
            "           CLEARWING MOTH       0.67      0.80      0.73         5\n",
            "               WOOD SATYR       1.00      0.20      0.33         5\n",
            "       TWO BARRED FLASHER       0.50      0.40      0.44         5\n",
            "  WHITE LINED SPHINX MOTH       0.38      0.60      0.46         5\n",
            "                LUNA MOTH       0.80      0.80      0.80         5\n",
            "               PINE WHITE       1.00      0.60      0.75         5\n",
            "       CHECQUERED SKIPPER       0.40      0.80      0.53         5\n",
            "   MILBERTS TORTOISESHELL       0.67      0.80      0.73         5\n",
            "         BROOKES BIRDWING       1.00      0.80      0.89         5\n",
            "             GREAT EGGFLY       0.29      0.40      0.33         5\n",
            "                 CHESTNUT       0.83      1.00      0.91         5\n",
            "   HUMMING BIRD HAWK MOTH       0.22      0.40      0.29         5\n",
            "             PAINTED LADY       1.00      0.40      0.57         5\n",
            "     EASTERN DAPPLE WHITE       0.25      0.40      0.31         5\n",
            "         PIPEVINE SWALLOW       0.75      0.60      0.67         5\n",
            "     ARCIGERA FLOWER MOTH       0.57      0.80      0.67         5\n",
            "                MALACHITE       0.50      0.40      0.44         5\n",
            "          GREY HAIRSTREAK       0.25      0.20      0.22         5\n",
            "       OLEANDER HAWK MOTH       0.00      0.00      0.00         5\n",
            "          ELBOWED PIERROT       0.50      0.60      0.55         5\n",
            "      SIXSPOT BURNET MOTH       0.60      0.60      0.60         5\n",
            "       GIANT LEOPARD MOTH       0.29      0.40      0.33         5\n",
            "              BLUE MORPHO       0.33      0.20      0.25         5\n",
            "           BROWN SIPROETA       0.80      0.80      0.80         5\n",
            "           STRAITED QUEEN       0.80      0.80      0.80         5\n",
            "AFRICAN GIANT SWALLOWTAIL       1.00      1.00      1.00         5\n",
            "             EASTERN COMA       0.36      0.80      0.50         5\n",
            "        BANDED TIGER MOTH       0.40      0.80      0.53         5\n",
            "                    AN 88       1.00      1.00      1.00         5\n",
            "            QUESTION MARK       0.25      0.20      0.22         5\n",
            "              RED CRACKER       0.36      0.80      0.50         5\n",
            "              RED POSTMAN       0.50      0.40      0.44         5\n",
            "                   ADONIS       0.60      0.60      0.60         5\n",
            "      YELLOW SWALLOW TAIL       0.60      0.60      0.60         5\n",
            "            CRIMSON PATCH       1.00      0.40      0.57         5\n",
            "                SOOTYWING       1.00      0.20      0.33         5\n",
            "            BECKERS WHITE       0.50      0.20      0.29         5\n",
            "      GLITTERING SAPPHIRE       0.56      1.00      0.71         5\n",
            "  BIRD CHERRY ERMINE MOTH       1.00      0.60      0.75         5\n",
            "        TROPICAL LEAFWING       0.00      0.00      0.00         5\n",
            "                METALMARK       0.60      0.60      0.60         5\n",
            "        PURPLE HAIRSTREAK       1.00      0.40      0.57         5\n",
            "          ORCHARD SWALLOW       0.50      0.40      0.44         5\n",
            "               ATLAS MOTH       1.00      0.80      0.89         5\n",
            "            HERCULES MOTH       0.57      0.80      0.67         5\n",
            "       CLODIUS PARNASSIAN       0.25      0.20      0.22         5\n",
            "         BLACK HAIRSTREAK       1.00      0.40      0.57         5\n",
            "          POLYPHEMUS MOTH       1.00      0.20      0.33         5\n",
            "                  MONARCH       1.00      0.40      0.57         5\n",
            "                CLEOPATRA       0.75      0.60      0.67         5\n",
            "                   MESTRA       0.00      0.00      0.00         5\n",
            "            CABBAGE WHITE       0.50      0.20      0.29         5\n",
            "          PURPLISH COPPER       0.18      0.40      0.25         5\n",
            "            SLEEPY ORANGE       0.38      0.60      0.46         5\n",
            "      SILVER SPOT SKIPPER       0.50      0.80      0.62         5\n",
            "         SOUTHERN DOGFACE       0.50      0.20      0.29         5\n",
            "          ROSY MAPLE MOTH       0.43      0.60      0.50         5\n",
            "           ORANGE OAKLEAF       0.50      0.40      0.44         5\n",
            "                    JULIA       0.50      0.20      0.29         5\n",
            "                   ULYSES       0.67      0.40      0.50         5\n",
            "          Iphiclus sister       0.29      0.40      0.33         5\n",
            "          CLOUDED SULPHUR       0.00      0.00      0.00         5\n",
            "           BANDED PEACOCK       1.00      0.80      0.89         5\n",
            "           SCARCE SWALLOW       0.60      0.60      0.60         5\n",
            "                 POPINJAY       1.00      0.40      0.57         5\n",
            "              COPPER TAIL       0.20      0.20      0.20         5\n",
            "            CINNABAR MOTH       1.00      0.80      0.89         5\n",
            "                  CRECENT       0.67      0.80      0.73         5\n",
            "\n",
            "                 accuracy                           0.53       500\n",
            "                macro avg       0.58      0.53      0.52       500\n",
            "             weighted avg       0.58      0.53      0.52       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "predict_x=model.predict(x_val) \n",
        "predictions=np.argmax(predict_x,axis=1)\n",
        "lll=y_val.tolist()\n",
        "llll=[]\n",
        "for x in range(len(predictions)):\n",
        "  vv=lll[x].index(1)\n",
        "  llll.append(vv)\n",
        "print(classification_report(llll, predictions,target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "151e74ed-1e14-4439-88db-99351208e26b",
      "metadata": {
        "id": "151e74ed-1e14-4439-88db-99351208e26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be68f39c-f30d-45c0-d67f-d20ab551666a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 10ms/step\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "               ORANGE TIP       0.25      0.20      0.22         5\n",
            "             LARGE MARBLE       0.00      0.00      0.00         5\n",
            "        COMMON WOOD-NYMPH       0.20      0.20      0.20         5\n",
            "          CHALK HILL BLUE       0.20      0.20      0.20         5\n",
            "               PAPER KITE       0.17      0.20      0.18         5\n",
            "                    ATALA       1.00      0.40      0.57         5\n",
            "         GREEN HAIRSTREAK       0.00      0.00      0.00         5\n",
            "        BLUE SPOTTED CROW       0.00      0.00      0.00         5\n",
            "       RED SPOTTED PURPLE       0.00      0.00      0.00         5\n",
            "              GOLD BANDED       0.40      0.40      0.40         5\n",
            "        GARDEN TIGER MOTH       1.00      0.20      0.33         5\n",
            "              RED ADMIRAL       0.50      0.60      0.55         5\n",
            "         MANGROVE SKIPPER       0.57      0.80      0.67         5\n",
            "            INDRA SWALLOW       0.00      0.00      0.00         5\n",
            "                  PEACOCK       0.50      0.20      0.29         5\n",
            "          CAIRNS BIRDWING       0.18      0.40      0.25         5\n",
            "               COMET MOTH       0.11      0.20      0.14         5\n",
            "                  IO MOTH       0.75      0.60      0.67         5\n",
            "              BROWN ARGUS       0.25      0.20      0.22         5\n",
            "                  APPOLLO       0.33      0.20      0.25         5\n",
            " GREEN CELLED CATTLEHEART       0.33      0.80      0.47         5\n",
            "                  VICEROY       0.15      0.40      0.22         5\n",
            "         EMPEROR GUM MOTH       0.00      0.00      0.00         5\n",
            "   MADAGASCAN SUNSET MOTH       0.36      0.80      0.50         5\n",
            "            DANAID EGGFLY       0.08      0.20      0.12         5\n",
            "        COMMON BANDED AWL       0.00      0.00      0.00         5\n",
            "          ZEBRA LONG WING       0.25      0.60      0.35         5\n",
            " BANDED ORANGE HELICONIAN       0.60      0.60      0.60         5\n",
            "                GREAT JAY       0.25      0.20      0.22         5\n",
            "           AMERICAN SNOOT       0.00      0.00      0.00         5\n",
            "           MOURNING CLOAK       0.75      0.60      0.67         5\n",
            "       EASTERN PINE ELFIN       0.00      0.00      0.00         5\n",
            "           CLEARWING MOTH       0.14      0.20      0.17         5\n",
            "               WOOD SATYR       0.00      0.00      0.00         5\n",
            "       TWO BARRED FLASHER       0.00      0.00      0.00         5\n",
            "  WHITE LINED SPHINX MOTH       0.12      0.20      0.15         5\n",
            "                LUNA MOTH       0.17      0.20      0.18         5\n",
            "               PINE WHITE       0.20      0.20      0.20         5\n",
            "       CHECQUERED SKIPPER       0.00      0.00      0.00         5\n",
            "   MILBERTS TORTOISESHELL       0.20      0.40      0.27         5\n",
            "         BROOKES BIRDWING       0.43      0.60      0.50         5\n",
            "             GREAT EGGFLY       0.29      0.40      0.33         5\n",
            "                 CHESTNUT       0.33      0.20      0.25         5\n",
            "   HUMMING BIRD HAWK MOTH       0.00      0.00      0.00         5\n",
            "             PAINTED LADY       0.00      0.00      0.00         5\n",
            "     EASTERN DAPPLE WHITE       0.00      0.00      0.00         5\n",
            "         PIPEVINE SWALLOW       0.50      0.20      0.29         5\n",
            "     ARCIGERA FLOWER MOTH       0.40      0.40      0.40         5\n",
            "                MALACHITE       0.00      0.00      0.00         5\n",
            "          GREY HAIRSTREAK       0.11      0.20      0.14         5\n",
            "       OLEANDER HAWK MOTH       0.50      0.20      0.29         5\n",
            "          ELBOWED PIERROT       0.40      0.40      0.40         5\n",
            "      SIXSPOT BURNET MOTH       0.40      0.40      0.40         5\n",
            "       GIANT LEOPARD MOTH       1.00      0.20      0.33         5\n",
            "              BLUE MORPHO       0.00      0.00      0.00         5\n",
            "           BROWN SIPROETA       0.50      0.20      0.29         5\n",
            "           STRAITED QUEEN       0.33      0.80      0.47         5\n",
            "AFRICAN GIANT SWALLOWTAIL       1.00      0.40      0.57         5\n",
            "             EASTERN COMA       0.00      0.00      0.00         5\n",
            "        BANDED TIGER MOTH       0.11      0.40      0.17         5\n",
            "                    AN 88       0.71      1.00      0.83         5\n",
            "            QUESTION MARK       0.33      0.20      0.25         5\n",
            "              RED CRACKER       0.75      0.60      0.67         5\n",
            "              RED POSTMAN       0.60      0.60      0.60         5\n",
            "                   ADONIS       0.33      0.20      0.25         5\n",
            "      YELLOW SWALLOW TAIL       0.38      0.60      0.46         5\n",
            "            CRIMSON PATCH       0.17      0.60      0.26         5\n",
            "                SOOTYWING       0.00      0.00      0.00         5\n",
            "            BECKERS WHITE       0.00      0.00      0.00         5\n",
            "      GLITTERING SAPPHIRE       0.14      0.20      0.17         5\n",
            "  BIRD CHERRY ERMINE MOTH       0.00      0.00      0.00         5\n",
            "        TROPICAL LEAFWING       0.00      0.00      0.00         5\n",
            "                METALMARK       0.67      0.40      0.50         5\n",
            "        PURPLE HAIRSTREAK       0.20      0.20      0.20         5\n",
            "          ORCHARD SWALLOW       0.50      0.40      0.44         5\n",
            "               ATLAS MOTH       0.00      0.00      0.00         5\n",
            "            HERCULES MOTH       0.00      0.00      0.00         5\n",
            "       CLODIUS PARNASSIAN       1.00      0.20      0.33         5\n",
            "         BLACK HAIRSTREAK       0.00      0.00      0.00         5\n",
            "          POLYPHEMUS MOTH       0.00      0.00      0.00         5\n",
            "                  MONARCH       0.43      0.60      0.50         5\n",
            "                CLEOPATRA       0.14      0.20      0.17         5\n",
            "                   MESTRA       0.00      0.00      0.00         5\n",
            "            CABBAGE WHITE       0.50      0.20      0.29         5\n",
            "          PURPLISH COPPER       0.00      0.00      0.00         5\n",
            "            SLEEPY ORANGE       0.00      0.00      0.00         5\n",
            "      SILVER SPOT SKIPPER       0.00      0.00      0.00         5\n",
            "         SOUTHERN DOGFACE       0.00      0.00      0.00         5\n",
            "          ROSY MAPLE MOTH       0.40      0.40      0.40         5\n",
            "           ORANGE OAKLEAF       1.00      0.20      0.33         5\n",
            "                    JULIA       0.25      0.20      0.22         5\n",
            "                   ULYSES       0.50      0.40      0.44         5\n",
            "          Iphiclus sister       0.44      0.80      0.57         5\n",
            "          CLOUDED SULPHUR       0.17      0.20      0.18         5\n",
            "           BANDED PEACOCK       0.25      0.20      0.22         5\n",
            "           SCARCE SWALLOW       1.00      0.20      0.33         5\n",
            "                 POPINJAY       0.29      0.80      0.42         5\n",
            "              COPPER TAIL       0.00      0.00      0.00         5\n",
            "            CINNABAR MOTH       0.25      0.40      0.31         5\n",
            "                  CRECENT       0.11      0.20      0.14         5\n",
            "\n",
            "                 accuracy                           0.26       500\n",
            "                macro avg       0.28      0.26      0.24       500\n",
            "             weighted avg       0.28      0.26      0.24       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "predict_x2=model2.predict(x_val) \n",
        "predictions2=np.argmax(predict_x2,axis=1)\n",
        "lll2=y_val.tolist()\n",
        "llll2=[]\n",
        "for x in range(len(predictions2)):\n",
        "  vv2=lll2[x].index(1)\n",
        "  llll2.append(vv2)\n",
        "print(classification_report(llll2, predictions2,target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the model reports above show, adding the dropout layer seems to have decreased the accuracy and not eliminated the overfitting problem. The most accurate outcome, at roughly 55%, was my first model, but the first and second models both suffered from overfitting. Below is my transfer learning model. This model performed much better than either of mine, but is considerably more robust and longer to train."
      ],
      "metadata": {
        "id": "ygceHEkUbD50"
      },
      "id": "ygceHEkUbD50"
    },
    {
      "cell_type": "code",
      "source": [
        "model3= keras.models.load_model('/content/archive-4/TransferModel.h5', custom_objects={'F1_score':'F1_score'})\n",
        "opt3 = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model3.compile(optimizer = opt3 , \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "history = model3.fit(x_train,y_train,epochs = 10 , validation_data = (x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEdsCDMeD-0",
        "outputId": "a2478045-4663-4534-8bcc-517552135b2b"
      },
      "id": "NKEdsCDMeD-0",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "394/394 [==============================] - 83s 176ms/step - loss: 2.1032 - accuracy: 0.7054 - val_loss: 1.4522 - val_accuracy: 0.8940\n",
            "Epoch 2/10\n",
            "394/394 [==============================] - 63s 161ms/step - loss: 1.3849 - accuracy: 0.8753 - val_loss: 1.3350 - val_accuracy: 0.9020\n",
            "Epoch 3/10\n",
            "394/394 [==============================] - 64s 162ms/step - loss: 1.1407 - accuracy: 0.9127 - val_loss: 1.0475 - val_accuracy: 0.9280\n",
            "Epoch 4/10\n",
            "394/394 [==============================] - 64s 163ms/step - loss: 1.0454 - accuracy: 0.9250 - val_loss: 1.0181 - val_accuracy: 0.9420\n",
            "Epoch 5/10\n",
            "394/394 [==============================] - 64s 163ms/step - loss: 1.1246 - accuracy: 0.9165 - val_loss: 1.0667 - val_accuracy: 0.9420\n",
            "Epoch 6/10\n",
            "394/394 [==============================] - 63s 161ms/step - loss: 0.9866 - accuracy: 0.9338 - val_loss: 1.1080 - val_accuracy: 0.9140\n",
            "Epoch 7/10\n",
            "394/394 [==============================] - 63s 161ms/step - loss: 0.9599 - accuracy: 0.9391 - val_loss: 1.1635 - val_accuracy: 0.9240\n",
            "Epoch 8/10\n",
            "394/394 [==============================] - 64s 162ms/step - loss: 1.0513 - accuracy: 0.9290 - val_loss: 0.9737 - val_accuracy: 0.9460\n",
            "Epoch 9/10\n",
            "394/394 [==============================] - 63s 161ms/step - loss: 0.8876 - accuracy: 0.9460 - val_loss: 1.0880 - val_accuracy: 0.9080\n",
            "Epoch 10/10\n",
            "394/394 [==============================] - 64s 162ms/step - loss: 0.9778 - accuracy: 0.9389 - val_loss: 1.1002 - val_accuracy: 0.9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x3=model3.predict(x_val) \n",
        "predictions3=np.argmax(predict_x3,axis=1)\n",
        "lll3=y_val.tolist()\n",
        "llll3=[]\n",
        "for x in range(len(predictions3)):\n",
        "  vv3=lll3[x].index(1)\n",
        "  llll3.append(vv3)\n",
        "print(classification_report(llll3, predictions3,target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o283EL8kcyX",
        "outputId": "3f7205c4-1898-428a-a935-2b065542f3cc"
      },
      "id": "4o283EL8kcyX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 45ms/step\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "               ORANGE TIP       1.00      0.80      0.89         5\n",
            "             LARGE MARBLE       1.00      0.80      0.89         5\n",
            "        COMMON WOOD-NYMPH       1.00      1.00      1.00         5\n",
            "          CHALK HILL BLUE       0.75      0.60      0.67         5\n",
            "               PAPER KITE       1.00      0.80      0.89         5\n",
            "                    ATALA       1.00      1.00      1.00         5\n",
            "         GREEN HAIRSTREAK       1.00      1.00      1.00         5\n",
            "        BLUE SPOTTED CROW       1.00      0.80      0.89         5\n",
            "       RED SPOTTED PURPLE       1.00      1.00      1.00         5\n",
            "              GOLD BANDED       1.00      1.00      1.00         5\n",
            "        GARDEN TIGER MOTH       0.71      1.00      0.83         5\n",
            "              RED ADMIRAL       0.83      1.00      0.91         5\n",
            "         MANGROVE SKIPPER       1.00      1.00      1.00         5\n",
            "            INDRA SWALLOW       1.00      0.60      0.75         5\n",
            "                  PEACOCK       1.00      1.00      1.00         5\n",
            "          CAIRNS BIRDWING       1.00      1.00      1.00         5\n",
            "               COMET MOTH       0.83      1.00      0.91         5\n",
            "                  IO MOTH       1.00      1.00      1.00         5\n",
            "              BROWN ARGUS       1.00      0.80      0.89         5\n",
            "                  APPOLLO       1.00      1.00      1.00         5\n",
            " GREEN CELLED CATTLEHEART       1.00      1.00      1.00         5\n",
            "                  VICEROY       1.00      1.00      1.00         5\n",
            "         EMPEROR GUM MOTH       0.62      1.00      0.77         5\n",
            "   MADAGASCAN SUNSET MOTH       1.00      1.00      1.00         5\n",
            "            DANAID EGGFLY       0.80      0.80      0.80         5\n",
            "        COMMON BANDED AWL       1.00      1.00      1.00         5\n",
            "          ZEBRA LONG WING       1.00      1.00      1.00         5\n",
            " BANDED ORANGE HELICONIAN       1.00      1.00      1.00         5\n",
            "                GREAT JAY       1.00      1.00      1.00         5\n",
            "           AMERICAN SNOOT       1.00      1.00      1.00         5\n",
            "           MOURNING CLOAK       1.00      1.00      1.00         5\n",
            "       EASTERN PINE ELFIN       0.80      0.80      0.80         5\n",
            "           CLEARWING MOTH       1.00      1.00      1.00         5\n",
            "               WOOD SATYR       0.83      1.00      0.91         5\n",
            "       TWO BARRED FLASHER       1.00      0.80      0.89         5\n",
            "  WHITE LINED SPHINX MOTH       0.83      1.00      0.91         5\n",
            "                LUNA MOTH       1.00      0.80      0.89         5\n",
            "               PINE WHITE       1.00      1.00      1.00         5\n",
            "       CHECQUERED SKIPPER       1.00      1.00      1.00         5\n",
            "   MILBERTS TORTOISESHELL       1.00      0.80      0.89         5\n",
            "         BROOKES BIRDWING       1.00      1.00      1.00         5\n",
            "             GREAT EGGFLY       1.00      1.00      1.00         5\n",
            "                 CHESTNUT       0.83      1.00      0.91         5\n",
            "   HUMMING BIRD HAWK MOTH       0.83      1.00      0.91         5\n",
            "             PAINTED LADY       1.00      1.00      1.00         5\n",
            "     EASTERN DAPPLE WHITE       1.00      0.80      0.89         5\n",
            "         PIPEVINE SWALLOW       1.00      1.00      1.00         5\n",
            "     ARCIGERA FLOWER MOTH       1.00      1.00      1.00         5\n",
            "                MALACHITE       1.00      1.00      1.00         5\n",
            "          GREY HAIRSTREAK       0.83      1.00      0.91         5\n",
            "       OLEANDER HAWK MOTH       1.00      1.00      1.00         5\n",
            "          ELBOWED PIERROT       1.00      1.00      1.00         5\n",
            "      SIXSPOT BURNET MOTH       1.00      1.00      1.00         5\n",
            "       GIANT LEOPARD MOTH       0.83      1.00      0.91         5\n",
            "              BLUE MORPHO       1.00      0.80      0.89         5\n",
            "           BROWN SIPROETA       1.00      1.00      1.00         5\n",
            "           STRAITED QUEEN       1.00      1.00      1.00         5\n",
            "AFRICAN GIANT SWALLOWTAIL       1.00      1.00      1.00         5\n",
            "             EASTERN COMA       0.57      0.80      0.67         5\n",
            "        BANDED TIGER MOTH       1.00      0.40      0.57         5\n",
            "                    AN 88       1.00      1.00      1.00         5\n",
            "            QUESTION MARK       1.00      0.60      0.75         5\n",
            "              RED CRACKER       1.00      1.00      1.00         5\n",
            "              RED POSTMAN       1.00      1.00      1.00         5\n",
            "                   ADONIS       0.67      0.40      0.50         5\n",
            "      YELLOW SWALLOW TAIL       0.60      0.60      0.60         5\n",
            "            CRIMSON PATCH       1.00      1.00      1.00         5\n",
            "                SOOTYWING       1.00      1.00      1.00         5\n",
            "            BECKERS WHITE       0.83      1.00      0.91         5\n",
            "      GLITTERING SAPPHIRE       1.00      1.00      1.00         5\n",
            "  BIRD CHERRY ERMINE MOTH       1.00      0.80      0.89         5\n",
            "        TROPICAL LEAFWING       0.80      0.80      0.80         5\n",
            "                METALMARK       1.00      1.00      1.00         5\n",
            "        PURPLE HAIRSTREAK       0.33      1.00      0.50         5\n",
            "          ORCHARD SWALLOW       1.00      1.00      1.00         5\n",
            "               ATLAS MOTH       1.00      1.00      1.00         5\n",
            "            HERCULES MOTH       1.00      1.00      1.00         5\n",
            "       CLODIUS PARNASSIAN       1.00      0.60      0.75         5\n",
            "         BLACK HAIRSTREAK       1.00      0.80      0.89         5\n",
            "          POLYPHEMUS MOTH       1.00      0.80      0.89         5\n",
            "                  MONARCH       1.00      1.00      1.00         5\n",
            "                CLEOPATRA       1.00      1.00      1.00         5\n",
            "                   MESTRA       0.80      0.80      0.80         5\n",
            "            CABBAGE WHITE       1.00      1.00      1.00         5\n",
            "          PURPLISH COPPER       1.00      0.20      0.33         5\n",
            "            SLEEPY ORANGE       1.00      0.80      0.89         5\n",
            "      SILVER SPOT SKIPPER       1.00      1.00      1.00         5\n",
            "         SOUTHERN DOGFACE       0.80      0.80      0.80         5\n",
            "          ROSY MAPLE MOTH       0.83      1.00      0.91         5\n",
            "           ORANGE OAKLEAF       1.00      1.00      1.00         5\n",
            "                    JULIA       1.00      1.00      1.00         5\n",
            "                   ULYSES       1.00      1.00      1.00         5\n",
            "          Iphiclus sister       1.00      1.00      1.00         5\n",
            "          CLOUDED SULPHUR       1.00      1.00      1.00         5\n",
            "           BANDED PEACOCK       1.00      1.00      1.00         5\n",
            "           SCARCE SWALLOW       1.00      1.00      1.00         5\n",
            "                 POPINJAY       1.00      1.00      1.00         5\n",
            "              COPPER TAIL       0.62      1.00      0.77         5\n",
            "            CINNABAR MOTH       1.00      1.00      1.00         5\n",
            "                  CRECENT       1.00      1.00      1.00         5\n",
            "\n",
            "                 accuracy                           0.92       500\n",
            "                macro avg       0.94      0.92      0.92       500\n",
            "             weighted avg       0.94      0.92      0.92       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model had a greater accuracy than both my other models combined at 94%. However, it takes about five times as long to train. Often, this is the tradeoff, and this knowledge can greatly affect which models I choose to train and what I choose to use for the application at hand.\n",
        "\n",
        "Overall, I found this project to be very rewarding to complete. Regardless of the fact that my models were greatly outperformed by the prepared model that came with the dataset, I still obtained an accuracy that I am quite happy with, and I look forward to using this knowledge in the future."
      ],
      "metadata": {
        "id": "ndWL5niKkmjh"
      },
      "id": "ndWL5niKkmjh"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}